= Trivadis Reference Architecture for Modern Data Analytics Solutions

The *Trivadis Reference Architecture for Modern Data Analytics Solutions* defines a generic, vendor-neutral and common blueprint for data lake architectures.
The Trivadis Reference Architecture provides different *Solutions Modules*, which engage with the different areas of a modern data analytics solution.
Each module is first defined in a vendor-neutral form and shows and describes the architectural building blocks, which are important and critical for the implementation of a solution module.
For each solution module, we then identify *one or more Scenarios*, which presents a solution module “in action”. In a scenario, for each of the architectural building blocks a concrete technology or product is selected. By that is shows, how a solution module can be implemented on corresponding platform. This has been done for both on-premises as well as for various cloud platforms.
Before we start and delve into the architecture discussion, let’s quickly review the guiding principles for the construction of this Trivadis Reference Architecture.

*	*Keep the solution as “vendor-neutral” as possible.* All services should be chosen so that the cloud-/vendor lock-in is minimal. By that, we remain flexible and would be able to change the platform later to another cloud vendor or migrate it to an on-premises solution if needed. Additionally, it is much easier to find knowledge on a generic topic than on a vendor-specific solution. To avoid vendor lock-in, services of the cloud vendor can still be chosen, but solutions, which are based on open source software, should be preferred over services based on proprietary solutions. By that, a cloud service from the current vendor could be replaced by a service from another vendor with minimal changes, if he offers the same open source solution. This is for example possible with technologies such as Spark, Hadoop and Kafka.
*	Creating an architecture for a *single information system*, which combines *operational* and *analytical* concerns.
// We should explain why
*	*Asynchronous interface* preferred over *synchronous interfaces*.
// We should explain why
*	*Event-based/-driven instead of Polling and Scheduling.* Whenever possible, the solution should communicate with events instead of using polling-based or period scheduling-based approaches for detecting status changes.
// We should explain why
*	Solution should support the *Separation of Storage and Compute* for cost-efficiency reasons. This makes it easier to only pay for (parallel) computing when it is necessary. This is especially interesting, as with all cloud vendors, the storage costs are a significantly lower than computing cost.
// We should explain, that this might cause lower performance compared to DAS dependent on the size of the cluster and the storage interface for the benefit of lower costs and more flexibility
*	Solution supports *Automation over Manual Coding and Processes*. Whenever possible, a tool chain should be used which supports model-driven development and allows for the generating of the various artefacts which recur multiple times. This applies both for code as well as processes.
// We should explain why
*	If there are various alternative offerings available, then a *Serverless offering should have priority over a Platform-as-a-Service (PaaS) offering* and a *Platform-as-a-Service (PaaS) should have priority over an Infrastructure-as-a-Service (IaaS) offering*. By that, the cloud vendor is in charge of the infrastructure, which simplifies operating such a platform a lot. Additionally, it is also more cost-efficient, as especially in a Serverless but often also in PaaS offerings, you only pay for the resources consumed by an application.
// We should explain how this interferes with "No Cloud LockIn" and when to use which way
*	Solution should allow for *keeping all the raw data* in a logically central place, but offer an answer for *storage tiering*, i.e. separating hot, cold and archive data to benefit from lower storage rates. The separation should be as transparent as possible to the user of the stored data. By keeping the raw data as long as possible, it is possible to still go back to raw data, if some new requirements pop-up in the future, which is not known today. By keeping the “passive” raw data as close as possible to the “active”, implementing data processing, which can re-process all raw data collected since the start of the system, is much easier to implement.
// Here, the WHY is well explained
*	Supports modern architecture concepts and approaches, such as Event Streaming, Event Sourcing and CQRS to implement loosely coupled applications (a.k.a. Microservices).

When implementing a concrete solution, not all of these principles are equally relevant and some might even contradict others. There is also always a trade-off to be made when applying the different principles. Implementing a Data Lake in the cloud should follow some other rules than implementing it on-premises.
The Trivadis Reference Architecture for Modern Data Analytics Solutions, which we will use as a benchmark in this assessment is shown in Figure 1.

.Trivadis Reference Architecture for Modern Data Analytics Solutions
image::images/trivadis-ref-architecture.png[alt="not bad.",width=1024,height=1024]

== Description of Layers and Building Blocks

The reference architecture consists of different layers, which group various architectural building blocks. Here a short description of all these layers and their building blocks:

// Some of these text blocks do not match the architecture any more (see "Big Data Processing" in the picture). How should we handle this?
* *Data Source* – this layer combines the various data sources to be integrated with the data lake through various channels. The integration can be based on bulk or streaming data, and in case of bulk data, the semantics can either be a recurring full-load or a delta load.
*	*Edge Computing* – an optional layer, which can be used if some (pre)processing needs be done near or at the data source, i.e. to minimize the data to be transferred to the lake and by that reducing the burden on the network. The following building blocks are available:
**	*(Bulk) Data Flow* – allows the development and execution of modern (big) data flows. These data flows can both deal with bulk data as well as streaming data. The idea is to provide data integration between the data sources and the event hub or data store. This building block allows to perform some transformations on the messages. Traditionally this has been called ETL processing and in fact the (Bulk) Data Flow building block has some resemblance to ETL.
**	*Event Hub* – allows the buffering of data and supports the principle of publish/subscribe, e.g. a producer is writing a message into the event hub, which can then be consumed by multiple consumers (subscribers).
**	*Stream Analytics* – allows the processing directly on the event stream (data in motion) and provides analytical operations such as aggregation over time windows, event pattern detection, …
**	*Object Store* – allows storing data in a key-value like structure in an efficient and cost-effective matter.
*	*Integration* – ensures integration between the various layers. Focus is on the integration of the data sources with the data storage in the data lake, but also on the efficient and prompt forwarding of events to the Real-Time Processing Layer. The following building blocks are available:
**	*Query Virtualization* – A way to query data across many systems without having to copy data into a central place but get the data from the source systems. If all the metadata and access to the underlying data is schematized and managed in one place, this is called a *Federated Database*
**	*Disk Service* – provides the capability of importing large datasets (which are too large for copying them over the network in a cost- and time-efficient manner) into the data lake using one or more transportable/shippable disks. This is especially interesting if the data source and the data lake are physically distributed, i.e. if for the data lake a cloud service is used.
**	*Change Data Capture* - Refers to the process or technology for identifying and capturing changes made to a database. Those changes can then be applied to another system directly or send to an message broker or Event Hub for further distribution.
**	*(Bulk) Data Flow* – allows the development and execution of modern (big) data flows. These data flows deal with bulk data and the main purpose is to forward these blocks of data to the Data Lake. This is very much the idea of the ETL tools used in traditional data warehousing. Some of these tools have been adapted to cover big data workloads, but there is also a bunch of tools which have been created specifically for the usage in a Data Lake scenario.
**	*Data Flow* – this building block allows the collecting and forwarding of single messages originating from an event stream. As with the Bulk Data Flow building block, it allows to perform Transformation on the messages.
**	*Message Broker* - a traditional messaging middleware to decouple systems through the usage of queues (1:1) or topics (1:many).
**	*Event Hub* – allows the buffering of data and supports the principle of publish/subscribe, e.g. a producer is writing a message into the event hub, which can then be consumed by multiple consumers (subscribers).
**	*Service Bus* – a building block from the area of the Enterprise Service Bus (ESB). Similar to the Process Data Flow building block, it also services the integration with traditional Enterprise Systems and less the processing and forwarding of high-volume and high-velocity event streams. For that the specific Data Flow building block should be used.
**	*Orchestration* – provides the possibility to model and execute processes. These processes can be triggered by messages arriving in the integration layer and a process itself can publish messages. By that it is also possible to model and execute data flows, similar to the Bulk Data Flow and Data Flow building blocks. But as Process Data Flow are mostly based on traditional infrastructures, they are usually much less scalable compared to the new products. Process Data Flow is more suited for integrating traditional Enterprise Systems with the data lake, to perform automatic actions based on results of the data lake.
**	*API Gateway* - An API gateway takes all API calls from clients, then routes them to the appropriate service implementation with request routing, composition, and protocol translation.
**	*Orchestration/Scheduler* – provides the capability for the periodic starting and orchestrating batch jobs.
*	*Data Lake / DWH* – provides storage capabilities for a large amount of data in a cost-efficient way. The fundamental building block of this layer is the storage. Storage is organized into zones, which categorized the stage the data is in.  The following zones have been identified and named:
**	*Transient Landing Zone* - an optional zone where the data lands before it is moved into the raw zone. Common in highly regulated environments where the data has to go through some initial quality check before it can be stored. Only limited access is provided.
**	*Archived Zone* – data which should be preserved, even after it has been fully processed, but which does not have to be available immediately. It will take a few hours for the data to be made online and available.
**	*Raw Zone* – the zone where the raw data is stored and kept as the original source data. Raw data can be optionally moved into an Archived Zones if no longer needed in regular processing.
**	*Refined Zone* – the zone where data is altered so that it follows all government and industry policies, as well as checked for quality. Standard data cleansing and data validation methods are performed here.
**	*Usage Optimised Zone* – manipulated and enriched data is kept in this zone. This data is prepared in a way that it can directly serve the data access from the consuming systems.
**	*Sandbox Zone* – this zone is primarily explored by data science teams. Provide the computing required for data scientists to tackle typically complex analytical workloads.
*	*Data Lake / Big Data Processing* – provides the capabilities for processing the data in the data lake in an efficient and scalable way so that the data can be transformed from one form into another. The following building blocks are available:
**	*Transform* - process of converting data or information from one format to another, usually from the format of a source system into the required format of a new destination system.
// If this should be the pendant of the Transformation in Data Warehousing, it is much more than just changing the format. It includes all operations required to integrate and historize the data like joins, set operations, splitter, filter, aggregations, partitioning, pivoting and unpivoting,  functions and much more. If this is NOT the pendant to DWH Transformation, where is it described?
**	*Enrichment* – is a general term that refers to processes used to enhance, refine or otherwise improve raw data. Data which is used for enrichment can be part of the data lake or can be read on-demand from outside the data lake.
**	*Aggregation* – also known as Consolidation, is a type of data and information mining process where data is searched, gathered and presented in summarized format to achieve specific business objectives or processes and/or conduct human analysis. Aggregation can be created “on-demand” or stored in materialized form.
**	*Cleansing/Validating* – the process of detecting and correcting (or removing) corrupt or inaccurate records from a data set and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, labeling, or deleting the dirty or coarse data.
**	*Event Handler* – provides a simple solution for reacting on a “storage event” in the Big Data Storage, i.e. when a new object/file is stored in one of the zones and to start some processing based on that event. One way for implementing this building block is in a «serverless» manner, where an infrastructure is in place, which allows to execute functions triggered by an event.
*	*Big Data Analytics* – provides the capabilities to perform Advanced Analytics on the data stored in the data in an efficient and scalable manner.
**	*Machine Learning* – uses algorithms to parse data, learn from that data and make informed decisions on what it has learnt.
**	*Image/Video Recognition* - the ability of software to identify objects, places, people, writing and actions in images and videos.
**	*Timeseries Analysis* - comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.
**	*Graph/Link Analytics* - a data analysis technique used in network theory that is used to evaluate the relationships or connections between network nodes
**	*Location Analytics* - the process or the ability to gain insight from the location or geographic component of business data.
*	*Big Data Federation* – provides the capability to access data stored in the data lake from external in a safe and simple manner. By that traditional, standard data analytics and visualization utilities can be used to access the data. The following building blocks are available:
**	*Query Engine* – provides the necessary functionality for accessing data by using a query language. The kind of query language is dependent on the underlying storage technology, but most often a SQL dialect is supported.
**	*API / Service* – the technique for providing access to data over clearly defined and governed interfaces. Today typically REST interfaces providing JSON data are in use with the potential extension with GraphQL.
// SQl is a clearly defined and governed interface too ;-)
*	*Real-Time Data Processing* – provides the capability to process the data while still in-motion and by that getting actionable insights with minimal latency. The following building blocks are available:
**	*Stream Analytics* - allows the processing directly on the event stream (data in motion) and provides analytical operations such as aggregation over time windows, event pattern detection, …
**	*Event Handler* - provides a simple solution for reacting on a new event in the event hub and to start some processing based on the event. One way for implementing this building block is in a «serverless» manner, where an infrastructure is in place, which allows to execute functions triggered by an event.
**	*Usage Optimized Data* – actionable insight which needs to available later can be stored here. This data is prepared in a way that it can directly serve the data access from the consuming systems.
**	*ML Model* – a machine learning trained model, which is expected to perform some intelligent stuff to be used from a pipeline implemented by the Stream Analytics building block.
*	*Enterprise Data Warehouse* – this layer is a placeholder for a traditional, existing Data Warehouse, which can be integrated with the data lake. The Data Warehouse bases on its existing Blueprints. The following building blocks are available:
**	*RDMBS* – a relational Database, which is often the base for the storage of the data in a Data Warehouse.
**	*Multi-Dimensional* – a specific type of database that has been optimized for data warehousing and OLAP (online analytical processing).
*	*Modern Apps (Microservices)* – provides an environment for developing and running applications which are served with data / results from the data lake. The following building blocks are available:
**	*Microservice* – an application with a clear bounded context, providing some specialized, fine-grained functionality mostly on the data/results stored in the data lake.
**	*App Marketplace* – a digital distribution platform for microservices-based applications.
**	*Usage-Optimized Data* - actionable insight which needs to be available in a read-only fashion by the microservices are kept here (materialized views). This data is prepared in a way that it can directly serve the data access from the consuming microservices.
*	*Information Consumer* - The following building blocks are available:
**	*Data Science Lab* - term for certain kinds of initial analysis and findings done with data sets, usually early on in an analytical process. It can be described as “taking a peek” at the data to understand more about what it represents and how to apply it.
**	*Batch Data Visualization* - the process of displaying stored data/information in graphical charts, figures, maps and bars.
**	*Streaming Data Visualization* - the process of displaying streaming as well as stored data/information in graphical charts, figures, maps and bars.
**	*Self-Service Analytics* - an approach to advanced analytics that allows business users to manipulate data to spot business opportunities, without requiring them to have a background in statistics or technology
*	*Master Data* – provides the relevant master and reference data to the data lake, which is often necessary in the processing and analytics of the data. The following building blocks are available:
**	*API / Service* - the technique for providing access to data over clearly defined and governed interfaces. Today typically REST interfaces providing JSON data are in use with the potential extension with GraphQL.
**	*Master Data* – the database persisting the master-/reference-data.
// Where is Metadata management / governance described?
*	*Enterprise Apps* – all existing, traditional applications of an enterprise. The following building blocks are available:
**	*API / Service* - the technique for providing access to data over clearly defined and governed interfaces.
**	*Data* – the data stored by an enterprise application, either as a file or in form of a relational database.
**	*Enterprise App* – any traditional, existing application or system used in an enterprise, such as an ERP solution.

The Trivadis Reference Architecture with its layers and building block offers a universal blueprint for analytics solutions. But not all data lake projects will need all the building blocks at the beginning or at all. To be able to communicate and decide what is needed for a given data lake project, various solution modules have been identified.

== Solution Modules

Each solution module bundles a set of requirements needed by a data lake. A solution module covers a specific aspect of a data lake solution.

The following table defines the 14 different solution modules we have identified so far.


[width="100%",cols="2,3,10",options="header"]
|===================================================
|ID |Module Name |Module Description
|M1 |Batch Data Ingestion |Supports the ingestion of bulk data into the data lake.
The end-to-end latency should not be the main concern here, the focus is on efficiency and throughput.
It’s of course the right fit if a data source can only provide bulk data, often by extracting data to a file.
|M2	|Stream Data Ingestion & Integration |Supports to tap events from a high-volume and high-velocity data stream. Here the focus is on minimal latency, to retrieve the events in a secure, efficient and reliable way and to forward the events as quick as possible (low latency). This module does not cover any storage and processing/analytics of the data, for that either M3 or M6 can be used.
|M3	|Big Data Storage |Supports long term storage of data of the Data Lake. Provides various storage technologies, opitimized for the various data access requirements.
|M4	|Big Data Preparation and Processing |Supports processing of the large datasets persisted in the various data stores of the Big Data Storage layer. The focus here is on efficient and scalable transformations of the data in the Big Data Storage layer from one zone to another/the next zone.
|M5	|Analytics and Machine Learning |Supports Advanced Analytics, such as Machine Learning, Natural Language Processing (NLP), Deep Learning, AI, … on the data stored in the data lake. The output, i.e. a ML Model is stored in the data lake and through that made available to the users of the model.
|M6	|Stream Analytics |Offers the processing of events directly on the data stream, without having to persist them before. The focus here is on velocity, a single event should be processed as quick as possible and actionable insights be available with minimal latency.
|M7	|Invoke External System	 |Allows the integration of traditional systems with the data lake.
Supports also the «way back» to a data source, to automatically adjust a data source. This is the case with IoT devices, where an actuator is in place, which can be invoked to control the device. The focus here is on the addressability of a single data source and the reliable communication with that data source from the data lake.
|M8	|Edge Processing |Provides the capabilities to run part of storage and analytics at the edge.
|M9	|Data Lake powered Applications |Provides a platform for developing, provisioning and running applications, which are primarily powered by data and results from the data lake. The focus here is on ease of use, both for the developer as well as for the end-user of the application.
|M10 |Pushing Data From Data Lake |Allows for exporting data from the data lake and by that provide it to other, external systems. In this solution module the data is pushed to the external system, whereas in M6 the data is pulled by the external systems.
|M11 |Access the Data Lake |Supports a standardized access of the data lying in the various zones in the Big Data Storage layer. The focus here is on simplicity of data access. This can be useful for traditional reporting, analytics or visualization tools as well as new applications being implemented on top of the data lake (provided my M10). In this solution module the data is pulled from the external systems, whereas in M7 the data is pushed from the data lake to the external systems.
|M12 |Data Federation and Virtualization |Provides functionality to allow for data from multiple data lakes and other data sources to be queried and combined. It is especially important for data which is hard to integrate.
|M13 |Orchestrating Data Lake |Provides functionality necessary for the plumbing associated with long-running processes, that is both needed for the M4 and M5. It handles chaining of tasks, their automation and the how to deal with failures. The tasks can be anything, like processing jobs, machine learning jobs, dumping data in and out of a database and much more. Triggers can be both time-based as well as event-based.
|M14 |Automating the Data Lake |Provides functionality for the automation of repetitive development tasks. This can be the generation of logic, such as data flows controlling the data ingestion or transformations of the data between the various zones. The focus is on reduction of manually copy/paste creation of artifacts, providing the capability for a global regenerate in case of fundamental changes to tools and their usage.
|M15 |Govern the Data Lake |Supports the necessary task for managing the data lake, such as the various aspects of Data Governance, i.e. managing assets in a Data Catalog, providing Data Lineage, … but also aspects of DataOps.
|M16 |Secure the Data Lake |Provides the necessary functionality to protect the data lake and the data stored within.
|M17 |Master Data Management |Provides functionality to manage master and reference data, which is needed for the functionality of a data lake.
|===================================================

Figure 2 shows these 14 solution modules in context of the *Trivadis Reference Architecture*.

.Trivadis Reference Architecture for Modern Data Analytics Solutions – different solution modules
image::images/trivadis-ref-architecture-solution-modules.png[alt="not bad.",width=1024,height=1024]

All modules are presented as an overlay on top of the diagram from Figure 1. This is no exact science; it should provide the reader an idea on the areas of the reference architecture which are covered by a given solution modules.

For each solution module, we have created a separate diagram, which only highlights parts which are needed from the Reference Architecture but in more detail. These diagrams are still vendor neutral and use the same basic building blocks from the Reference Architecture.

.Trivadis Reference Architecture for Modern Data Analytics Solutions – a diagram per solution module
image::images/trivadis-ref-architecture-solution-modules2.png[alt="not bad.",width=1024,height=1024]

In the next section we delve into some options for adapting from a traditional, batch-based data lake with high latency into a more reactive data lake, updated in near real-time.

== Mapping to Cloud Services

The Trivadis Reference Architecture can be implemented on-premises using various Vendor and Open Source technologies as well as in the cloud.

In this section we visually show the mapping of AWS and Azure cloud services to the Reference Architecture.

Figure 4 shows the mapping of AWS services to the Reference Architecture.

.Trivadis Reference Architecture – Mapping to AWS Services
image::images/trivadis-ref-architecture-aws-mapping.png[alt="not bad.",width=1024,height=1024]

Figure 5 shows the mapping of Azure services to the Reference Architecture.

.Trivadis Reference Architecture – Mapping to Azure Services
image::images/trivadis-ref-architecture-azure-mapping.png[alt="not bad.",width=1024,height=1024]

Figure 6 shows the mapping of Oracle services to the Reference Architecture.

.Trivadis Reference Architecture – Mapping to Azure Services
image::images/trivadis-ref-architecture-oracle-mapping.png[alt="not bad.",width=1024,height=1024]

// Don't we need an On-Prem Mapping too?
